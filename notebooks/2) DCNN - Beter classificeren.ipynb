{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beter verkeersborden herkennen: Het DCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Neural Networks (DNN's) zijn neurale netwerken met veel lagen. Deep Convolutional Neural Networks (DCNN's) zijn diepe netwerken ontworpen om convoluties (lokale transformatie functies) te leren. Deze laatste netwerken zijn op het moment de state of the art. \n",
    "\n",
    "![Convolutie](images/convolution-calculate.png)\n",
    "\n",
    "Door convoluties te stapelen kan een netwerk als het filters leren voor complexe objecten (Bijvoorbeeld: Een stoel is een combinatie van stoelpoten en een zetel, een stoelpoot is een combinatie van rechte evenwijdige lijnen)\n",
    "\n",
    "![Filters](images/Hl2H6.png)\n",
    "\n",
    "## Tweede poging: Verkeersborden classificeren\n",
    "In deel I hebben we een netwerk gebouwd dat rond de 65% van de borden herkent. Het netwerk leerde snel, maar de prestaties liepen wat uiteen.\n",
    "\n",
    "Je zal zien dat met een paar kleine ingrepen we aardig beter gaan presteren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stijn/anaconda2/envs/tensortut/lib/python3.5/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import skimage.transform\n",
    "import skimage.data\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Allow image embeding in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "def load_data(data_dir):\n",
    "    \"\"\"Loads a data set and returns two lists:\n",
    "    \n",
    "    images: a list of Numpy arrays, each representing an image.\n",
    "    labels: a list of numbers that represent the images labels.\n",
    "    \"\"\"\n",
    "    # Get all subdirectories of data_dir. Each represents a label.\n",
    "    directories = [d for d in os.listdir(data_dir) \n",
    "                   if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    # Loop through the label directories and collect the data in\n",
    "    # two lists, labels and images.\n",
    "    labels = []\n",
    "    images = []\n",
    "    for d in directories:\n",
    "        label_dir = os.path.join(data_dir, d)\n",
    "        file_names = [os.path.join(label_dir, f) \n",
    "                      for f in os.listdir(label_dir) if f.endswith(\".ppm\")]\n",
    "        # For each label, load it's images and add them to the images list.\n",
    "        # And add the label number (i.e. directory name) to the labels list.\n",
    "        for f in file_names:\n",
    "            images.append(skimage.data.imread(f))\n",
    "            labels.append(int(d))\n",
    "    images32 = [skimage.transform.resize(image, (32, 32)) for image in images]\n",
    "    return images32, labels\n",
    "\n",
    "# Load training and testing datasets.\n",
    "ROOT_PATH = \"./\"\n",
    "train_data_dir = os.path.join(ROOT_PATH, \"datasets/BelgiumTS/Training\")\n",
    "test_data_dir = os.path.join(ROOT_PATH, \"datasets/BelgiumTS/Testing\")\n",
    "\n",
    "images, labels = load_data(train_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Het DCNN\n",
    "Een DCNN is in feite een neuraal netwerk met meer dan 2 tussenlagen met minstens een convolutielaag. Maar daarnaast zijn er nog genoeg kleine aanpassingen die het model beter maken, zoals: Tussentijdse pooling en dropout layers. \n",
    "\n",
    "Eerst maken we een paar utilfuncties die ons straks gaan helpen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create this model, we're going to need to create a lot of weights and biases. \n",
    "#One should generally initialize weights with a small amount of noise for symmetry breaking, \n",
    "#and to prevent 0 gradients. Since we're using ReLU neurons, \n",
    "#it is also good practice to initialize them with a slightly positive initial bias to avoid \"dead neurons\". \n",
    "#Instead of doing this repeatedly while we build the model, let's create two handy functions to do it for us.\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#TensorFlow also gives us a lot of flexibility in convolution and pooling operations. \n",
    "#How do we handle the boundaries? What is our stride size? In this example, \n",
    "#we're always going to choose the vanilla version. \n",
    "#Our convolutions uses a stride of one and are zero padded so that the output is the same size as the input. \n",
    "#Our pooling is plain old max pooling over 2x2 blocks. To keep our code cleaner, \n",
    "#let's also abstract those operations into functions.\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def indices_to_one_hot(data, nb_classes):\n",
    "    \"\"\"Convert an iterable of indices to one-hot encoded labels.\"\"\"\n",
    "    targets = np.array(data).reshape(-1)\n",
    "    return np.eye(nb_classes)[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:  (4575, 62) \n",
      "images:  (4575, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "labels_a = indices_to_one_hot(labels,62)\n",
    "images_a = np.array(images)\n",
    "print(\"labels: \", labels_a.shape, \"\\nimages: \", images_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Dan definieren we het deep net, zoals hier onder weergegeven:\n",
    "        \n",
    "![DCNN](images/mnist_deep.png)\n",
    "        \n",
    "Althans dit wordt gezien als een bescheiden diep netwerk..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph to hold the model.\n",
    "graph = tf.Graph()\n",
    "\n",
    "# Create model in the graph.\n",
    "with graph.as_default():\n",
    "    # Placeholders for inputs and labels.\n",
    "    images_ph = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    labels_ph = tf.placeholder(tf.int32, [None,62])\n",
    "    images = tf.reshape(images_ph, [-1, 32, 32, 3])\n",
    "    \n",
    "    #two conv & pooling pairs\n",
    "    h_conv1 = tf.nn.relu(conv2d(images, weight_variable([3, 3, 3, 32])) + bias_variable([32]))\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, weight_variable([5, 5, 32, 64])) + bias_variable([64]))\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "    \n",
    "    #Now that the image size has been reduced to 8x8, we add a fully-connected layer with 1024 neurons \n",
    "    #to allow processing on the entire image. \n",
    "    #We reshape the tensor from the pooling layer into a batch of vectors, \n",
    "    #multiply by a weight matrix, add a bias, and apply a ReLU.\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, weight_variable([8 * 8 * 64, 1024])) + bias_variable([1024]))\n",
    "\n",
    "    #To reduce overfitting, we will apply dropout before the readout layer. \n",
    "    #We create a placeholder for the probability that a neuron's output is kept during dropout. \n",
    "    #This allows us to turn dropout on during training, and turn it off during testing. \n",
    "    #TensorFlow's tf.nn.dropout op automatically handles scaling neuron outputs in addition to masking them, \n",
    "    #so dropout just works without any additional scaling.\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    #Finally, we add a layer for softmax regression.\n",
    "    y_conv = tf.matmul(h_fc1_drop, weight_variable([1024, 62])) + bias_variable([62])    \n",
    "    \n",
    "    #variables for training and evaluation\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels_ph, logits=y_conv))\n",
    "    train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(labels_ph, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))   \n",
    "    \n",
    "    init = tf.initialize_all_variables()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We gaan het model nu 200 keer trainen. (We gebruiken de ADAM optimizer om de cross-entropy zo laag mogelijk te krijgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a session to run the graph we created.\n",
    "session = tf.Session(graph=graph)\n",
    "\n",
    "# First step is always to initialize all variables. \n",
    "# We don't care about the return value, though. It's None.\n",
    "_ = session.run([init])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(201): #201\n",
    "    _, loss_value = session.run([train, cross_entropy], \n",
    "                                feed_dict={images_ph: images_a, labels_ph: labels_a, keep_prob: 0.5})\n",
    "    if i % 30 == 0:\n",
    "        print(\"Loss: \", loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluatie\n",
    "\n",
    "Eens kijken of we nu beter scoren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stijn/anaconda2/envs/tensortut/lib/python3.5/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth:        38 Prediction: 39\n",
      "Truth:        38 Prediction: 39\n",
      "Truth:        38 Prediction: 39\n",
      "Truth:        38 Prediction: 39\n",
      "Truth:        38 Prediction: 40\n",
      "Truth:        38 Prediction: 40\n",
      "Truth:        38 Prediction: 35\n",
      "Truth:        23 Prediction: 28\n",
      "Truth:        23 Prediction: 28\n",
      "Truth:        23 Prediction: 28\n",
      "Truth:        23 Prediction: 28\n",
      "Truth:        23 Prediction: 32\n",
      "Truth:        23 Prediction: 28\n",
      "Truth:        14 Prediction: 16\n",
      "Truth:        14 Prediction: 7\n",
      "Truth:        17 Prediction: 7\n",
      "Truth:        17 Prediction: 7\n",
      "Truth:        16 Prediction: 14\n",
      "Truth:        16 Prediction: 18\n",
      "Truth:        16 Prediction: 14\n",
      "Truth:        16 Prediction: 18\n",
      "Truth:        16 Prediction: 14\n",
      "Truth:        16 Prediction: 18\n",
      "Truth:        16 Prediction: 14\n",
      "Truth:        16 Prediction: 18\n",
      "Truth:        56 Prediction: 57\n",
      "Truth:        39 Prediction: 38\n",
      "Truth:        3 Prediction: 13\n",
      "Truth:        3 Prediction: 13\n",
      "Truth:        3 Prediction: 13\n",
      "Truth:        58 Prediction: 35\n",
      "Truth:        58 Prediction: 35\n",
      "Truth:        58 Prediction: 35\n",
      "Truth:        58 Prediction: 35\n",
      "Truth:        46 Prediction: 45\n",
      "Truth:        46 Prediction: 47\n",
      "Truth:        46 Prediction: 45\n",
      "Truth:        46 Prediction: 45\n",
      "Truth:        42 Prediction: 40\n",
      "Truth:        42 Prediction: 40\n",
      "Truth:        42 Prediction: 40\n",
      "Truth:        5 Prediction: 3\n",
      "Truth:        5 Prediction: 3\n",
      "Truth:        6 Prediction: 7\n",
      "Truth:        6 Prediction: 18\n",
      "Truth:        6 Prediction: 7\n",
      "Truth:        6 Prediction: 7\n",
      "Truth:        6 Prediction: 4\n",
      "Truth:        6 Prediction: 7\n",
      "Truth:        45 Prediction: 46\n",
      "Truth:        45 Prediction: 46\n",
      "Truth:        45 Prediction: 50\n",
      "Truth:        45 Prediction: 47\n",
      "Truth:        45 Prediction: 47\n",
      "Truth:        45 Prediction: 46\n",
      "Truth:        45 Prediction: 46\n",
      "Truth:        45 Prediction: 46\n",
      "Truth:        45 Prediction: 47\n",
      "Truth:        57 Prediction: 56\n",
      "Truth:        57 Prediction: 53\n",
      "Truth:        57 Prediction: 56\n",
      "Truth:        57 Prediction: 56\n",
      "Truth:        57 Prediction: 56\n",
      "Truth:        57 Prediction: 56\n",
      "Truth:        2 Prediction: 13\n",
      "Truth:        2 Prediction: 14\n",
      "Truth:        2 Prediction: 13\n",
      "Truth:        2 Prediction: 18\n",
      "Truth:        2 Prediction: 13\n",
      "Truth:        2 Prediction: 13\n",
      "Truth:        7 Prediction: 18\n",
      "Truth:        7 Prediction: 18\n",
      "Truth:        7 Prediction: 18\n",
      "Truth:        7 Prediction: 18\n",
      "Truth:        7 Prediction: 18\n",
      "Truth:        0 Prediction: 1\n",
      "Truth:        0 Prediction: 1\n",
      "Truth:        0 Prediction: 1\n",
      "Truth:        0 Prediction: 1\n",
      "Truth:        13 Prediction: 32\n",
      "Truth:        13 Prediction: 56\n",
      "Truth:        13 Prediction: 32\n",
      "Truth:        20 Prediction: 24\n",
      "Truth:        18 Prediction: 17\n",
      "Truth:        18 Prediction: 7\n",
      "Truth:        18 Prediction: 8\n",
      "Truth:        18 Prediction: 17\n",
      "Truth:        18 Prediction: 17\n",
      "Truth:        18 Prediction: 10\n",
      "Truth:        18 Prediction: 10\n",
      "Truth:        18 Prediction: 8\n",
      "Truth:        18 Prediction: 10\n",
      "Truth:        43 Prediction: 42\n",
      "Truth:        43 Prediction: 42\n",
      "Truth:        43 Prediction: 42\n",
      "Truth:        43 Prediction: 42\n",
      "Truth:        43 Prediction: 42\n",
      "Truth:        43 Prediction: 42\n",
      "Truth:        53 Prediction: 39\n",
      "Truth:        21 Prediction: 28\n",
      "Truth:        21 Prediction: 22\n",
      "Truth:        21 Prediction: 22\n",
      "Truth:        24 Prediction: 25\n",
      "Truth:        24 Prediction: 25\n",
      "Truth:        24 Prediction: 25\n",
      "Truth:        4 Prediction: 8\n",
      "Truth:        4 Prediction: 18\n",
      "Truth:        4 Prediction: 7\n",
      "Truth:        4 Prediction: 18\n",
      "Truth:        4 Prediction: 18\n",
      "Truth:        4 Prediction: 18\n",
      "Truth:        55 Prediction: 56\n",
      "Truth:        8 Prediction: 13\n",
      "Truth:        8 Prediction: 18\n",
      "Truth:        8 Prediction: 13\n",
      "Truth:        8 Prediction: 4\n",
      "Truth:        8 Prediction: 3\n",
      "Truth:        60 Prediction: 61\n",
      "Truth:        60 Prediction: 21\n",
      "Truth:        60 Prediction: 61\n",
      "Truth:        60 Prediction: 61\n",
      "Truth:        60 Prediction: 21\n",
      "Truth:        49 Prediction: 47\n",
      "Truth:        49 Prediction: 47\n",
      "Truth:        59 Prediction: 56\n",
      "Truth:        27 Prediction: 32\n",
      "Truth:        27 Prediction: 32\n",
      "Truth:        27 Prediction: 24\n",
      "Truth:        27 Prediction: 32\n",
      "Truth:        12 Prediction: 7\n",
      "Truth:        12 Prediction: 18\n",
      "Truth:        12 Prediction: 18\n",
      "Truth:        10 Prediction: 22\n",
      "Truth:        10 Prediction: 7\n",
      "Truth:        10 Prediction: 14\n",
      "Truth:        10 Prediction: 14\n",
      "Truth:        10 Prediction: 14\n",
      "Truth:        10 Prediction: 17\n",
      "Truth:        32 Prediction: 14\n",
      "Truth:        32 Prediction: 19\n",
      "Truth:        35 Prediction: 38\n",
      "Truth:        35 Prediction: 38\n",
      "Truth:        35 Prediction: 38\n",
      "Truth:        35 Prediction: 34\n",
      "Truth:        35 Prediction: 38\n",
      "Truth:        35 Prediction: 38\n",
      "Truth:        35 Prediction: 38\n",
      "Truth:        35 Prediction: 38\n",
      "Truth:        35 Prediction: 38\n",
      "Truth:        35 Prediction: 28\n",
      "Truth:        35 Prediction: 38\n",
      "Truth:        35 Prediction: 38\n",
      "Truth:        35 Prediction: 38\n",
      "Truth:        35 Prediction: 56\n",
      "Truth:        35 Prediction: 38\n",
      "Truth:        35 Prediction: 38\n",
      "Truth:        35 Prediction: 38\n",
      "Truth:        35 Prediction: 38\n",
      "Truth:        35 Prediction: 38\n",
      "Truth:        35 Prediction: 38\n",
      "Truth:        35 Prediction: 54\n",
      "Truth:        35 Prediction: 38\n",
      "Truth:        35 Prediction: 38\n",
      "Truth:        35 Prediction: 38\n",
      "Truth:        35 Prediction: 38\n",
      "Truth:        35 Prediction: 38\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkSgKfiESRcEvRKL8fxxz+L8xlS8RAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f71eed1b5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the test dataset.\n",
    "test_images, test_labels = load_data(test_data_dir)\n",
    "#test_labels_1h =  indices_to_one_hot(test_labels,62)\n",
    "\n",
    "predicted = session.run([tf.argmax(y_conv,1)], \n",
    "                        feed_dict={images_ph: test_images, keep_prob: 1})[0]\n",
    "\n",
    "for i in range(len(predicted)):\n",
    "    if predicted[i] != test_labels[i]:\n",
    "        plt.imshow(test_images[i])\n",
    "        print(\"Truth:        {0} Prediction: {1}\".format(test_labels[i], predicted[i]))\n",
    "    #truth = sample_labels[i]\n",
    "    #prediction = predicted[i]\n",
    "    #plt.subplot(5, 2,1+i)\n",
    "    #plt.axis('off')\n",
    "    #color='green' if truth == prediction else 'red'\n",
    "    #plt.text(40, 10, \"Truth:        {0}\\nPrediction: {1}\".format(truth, prediction), \n",
    "    #         fontsize=12, color=color)\n",
    "    #plt.imshow(sample_images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stijn/anaconda2/envs/tensortut/lib/python3.5/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.934127\n"
     ]
    }
   ],
   "source": [
    "# Load the test dataset.\n",
    "test_images, test_labels = load_data(test_data_dir)\n",
    "test_labels =  indices_to_one_hot(test_labels,62)\n",
    "\n",
    "# Run predictions against the full test set.\n",
    "acc = session.run([accuracy], feed_dict={images_ph: test_images, labels_ph: test_labels, keep_prob: 1.0})[0]\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Close the session. This will destroy the trained model.\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
